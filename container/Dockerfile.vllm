# -----------------------------------------------------------------------------
# This Dockerfile supports building for both x86_64 and ARM64 via ARG ARCH.
# Default is x86_64 (amd64). For ARM64, pass:
#   --build-arg ARCH=arm64 --platform=linux/arm64
# -----------------------------------------------------------------------------


########################################
#        Architecture Arguments
########################################

ARG ARCH=amd64

#
# x86_64 defaults
#
ARG BASE_IMAGE_X86="nvcr.io/nvidia/cuda-dl-base"
ARG BASE_IMAGE_TAG_X86="25.01-cuda12.8-devel-ubuntu24.04"
ARG RUNTIME_IMAGE_X86="nvcr.io/nvidia/cuda"
ARG RUNTIME_IMAGE_TAG_X86="12.8.1-runtime-ubuntu24.04"
ARG MANYLINUX_IMAGE_X86="quay.io/pypa/manylinux_2_28_x86_64"

# ARM64 defaults
ARG BASE_IMAGE_ARM="nvcr.io/nvidia/pytorch"
ARG BASE_IMAGE_TAG_ARM="25.03-py3"
ARG RUNTIME_IMAGE_ARM="nvcr.io/nvidia/pytorch"
ARG RUNTIME_IMAGE_TAG_ARM="25.03-py3"
ARG MANYLINUX_IMAGE_ARM="quay.io/pypa/manylinux_2_28_aarch64"

#
# Shared arguments
#
ARG MOFED_VERSION=24.10-1.1.4.0
ARG RELEASE_BUILD
ARG GENAI_PERF_TAG="e67e853413a07a778dd78a55e299be7fba9c9c24"
ARG NSYS_URL_X86="https://developer.nvidia.com/downloads/assets/tools/secure/nsight-systems/2025_1/"
ARG NSYS_PKG_X86="NsightSystems-linux-cli-public-2025.1.1.131-3554042.deb"
ARG PYTHON_VERSION=3.12

#
# We do final selections for base / runtime / manylinux based on $ARCH
#
### SELECT FINAL BASE + RUNTIME + MANYLINUX
FROM scratch AS arch_selector

ARG ARCH
ARG BASE_IMAGE_X86
ARG BASE_IMAGE_TAG_X86
ARG RUNTIME_IMAGE_X86
ARG RUNTIME_IMAGE_TAG_X86
ARG MANYLINUX_IMAGE_X86
ARG BASE_IMAGE_ARM
ARG BASE_IMAGE_TAG_ARM
ARG RUNTIME_IMAGE_ARM
ARG RUNTIME_IMAGE_TAG_ARM
ARG MANYLINUX_IMAGE_ARM

ENV SELECTED_BASE_IMAGE=""
ENV SELECTED_BASE_IMAGE_TAG=""
ENV SELECTED_RUNTIME_IMAGE=""
ENV SELECTED_RUNTIME_IMAGE_TAG=""
ENV SELECTED_MANYLINUX_IMAGE=""

SHELL ["/bin/sh", "-c"]
# We store the selected image info in environment variables:
RUN if [ "${ARCH}" = "arm64" ]; then \
      echo "ARM64 build. Using PyTorch 25.03 for base." && \
      echo "SELECTED_BASE_IMAGE=${BASE_IMAGE_ARM}"               >> /arch_env && \
      echo "SELECTED_BASE_IMAGE_TAG=${BASE_IMAGE_TAG_ARM}"       >> /arch_env && \
      echo "SELECTED_RUNTIME_IMAGE=${RUNTIME_IMAGE_ARM}"         >> /arch_env && \
      echo "SELECTED_RUNTIME_IMAGE_TAG=${RUNTIME_IMAGE_TAG_ARM}" >> /arch_env && \
      echo "SELECTED_MANYLINUX_IMAGE=${MANYLINUX_IMAGE_ARM}"      >> /arch_env ; \
    else \
      echo "x86_64 build. Using CUDA DL Base 25.01 for base." && \
      echo "SELECTED_BASE_IMAGE=${BASE_IMAGE_X86}"               >> /arch_env && \
      echo "SELECTED_BASE_IMAGE_TAG=${BASE_IMAGE_TAG_X86}"       >> /arch_env && \
      echo "SELECTED_RUNTIME_IMAGE=${RUNTIME_IMAGE_X86}"         >> /arch_env && \
      echo "SELECTED_RUNTIME_IMAGE_TAG=${RUNTIME_IMAGE_TAG_X86}" >> /arch_env && \
      echo "SELECTED_MANYLINUX_IMAGE=${MANYLINUX_IMAGE_X86}"      >> /arch_env ; \
    fi

# We'll re-stage from scratch below, pulling the environment variables.
FROM scratch AS arch_values
COPY --from=arch_selector /arch_env /arch_env

# We now read them in each subsequent stage.

#####################################################################
# Stage 1: "nixl_base" – for copying NIXL from an external location
#####################################################################
FROM scratch AS nixl
# This is presumably some pre-built or external container that has the NIXL source
# We keep the same naming as your original Dockerfile (above).
# You can also do: "FROM <nixl-repo>:<tag>" if you actually have that image

##################################
########## NIXL BASE #############
##################################

FROM scratch AS nixl_base
ARG NIXL_COMMIT
COPY --from=nixl / /opt/nixl   # the entire NIXL from your existing 'nixl' image
WORKDIR /opt/nixl
RUN echo "NIXL commit: ${NIXL_COMMIT}" > /opt/nixl/commit.txt

#####################################################################
# Stage 2: "base" – the main dev base environment
#####################################################################

FROM busybox as env_reader
COPY --from=arch_values /arch_env /arch_env

# We read environment from /arch_env so we can do FROM dynamic references
ARG SELECTED_BASE_IMAGE
ARG SELECTED_BASE_IMAGE_TAG

FROM ${SELECTED_BASE_IMAGE}:${SELECTED_BASE_IMAGE_TAG} AS base

USER root

# Re-declare build args for use in this stage:
ARG ARCH
ARG MOFED_VERSION
ARG PYTHON_VERSION
ARG NSYS_URL_X86
ARG NSYS_PKG_X86
ARG NIXL_COMMIT

# Show debug info
RUN echo "Building 'base' stage for ARCH=$ARCH" && uname -m

### NIXL SETUP ###

RUN apt-get update -y && apt-get -y install curl \
    git \
    libnuma-dev \
    numactl \
    wget \
    autotools-dev \
    automake \
    libtool \
    libz-dev \
    libiberty-dev \
    flex \
    build-essential \
    cmake \
    libibverbs-dev \
    libgoogle-glog-dev \
    libgtest-dev \
    libjsoncpp-dev \
    libpython3-dev \
    libboost-all-dev \
    libssl-dev \
    libgrpc-dev \
    libgrpc++-dev \
    libprotobuf-dev \
    libclang-dev \
    protobuf-compiler-grpc \
    pybind11-dev \
    python3-full \
    python3-pip \
    python3-numpy \
    etcd-server \
    net-tools \
    pciutils \
    libpci-dev \
    vim \
    tmux \
    screen \
    ibverbs-utils \
    libibmad-dev \
    linux-tools-common \
    linux-tools-generic \
    ethtool \
    iproute2 \
    dkms \
    linux-headers-generic \
    meson \
    ninja-build \
    uuid-dev \
    gdb \
    libglib2.0-0

# Nsight Systems: only works for x86_64. On ARM we skip
RUN if [ "$ARCH" = "amd64" ]; then \
      echo "Installing Nsight Systems for x86_64..." && \
      wget ${NSYS_URL_X86}${NSYS_PKG_X86} && \
      apt install -y ./${NSYS_PKG_X86} && \
      rm ${NSYS_PKG_X86}; \
    else \
      echo "Skipping Nsight Systems for ARM64..."; \
    fi

# MOFED version differs: x86_64 vs. aarch64 tar name
RUN cd /usr/local/src && \
    if [ "$ARCH" = "amd64" ]; then \
      echo "Download MOFED x86_64" && \
      curl -fSsL "https://content.mellanox.com/ofed/MLNX_OFED-${MOFED_VERSION}/MLNX_OFED_LINUX-${MOFED_VERSION}-ubuntu24.04-x86_64.tgz" -o mofed.tgz ; \
    else \
      echo "Download MOFED aarch64" && \
      curl -fSsL "https://content.mellanox.com/ofed/MLNX_OFED-${MOFED_VERSION}/MLNX_OFED_LINUX-${MOFED_VERSION}-ubuntu24.04-aarch64.tgz" -o mofed.tgz ; \
    fi && \
    tar -xf /usr/local/src/mofed.tgz && \
    cd MLNX_OFED_LINUX-* && \
    apt-get update && apt-get install -y --no-install-recommends \
       ./DEBS/libibverbs* ./DEBS/ibverbs-providers* ./DEBS/librdmacm* ./DEBS/libibumad* && \
    rm -rf /var/lib/apt/lists/* /usr/local/src/* mofed.tgz

ENV LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda/lib64 \
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64
ENV LIBRARY_PATH=$LIBRARY_PATH:/usr/local/lib \
    LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib

WORKDIR /workspace
RUN git clone https://github.com/NVIDIA/gdrcopy.git
RUN PREFIX=/usr/local DESTLIB=/usr/local/lib make -C /workspace/gdrcopy lib_install

# The library location differs on x86 vs. arm
RUN if [ "$ARCH" = "amd64" ]; then \
      cp gdrcopy/src/libgdrapi.so.2.* /usr/lib/x86_64-linux-gnu/; \
    else \
      cp gdrcopy/src/libgdrapi.so.2.* /usr/lib/aarch64-linux-gnu/; \
    fi && \
    ldconfig

#
# Build UCX from source (same for both x86 & arm as long as it compiles)
#
ARG UCX_VERSION=v1.18.0
RUN cd /usr/local/src && \
    curl -fSsL "https://github.com/openucx/ucx/tarball/${UCX_VERSION}" | tar xz && \
    cd openucx-ucx* && \
    ./autogen.sh && ./configure \
       --enable-shared \
       --disable-static \
       --disable-doxygen-doc \
       --enable-optimizations \
       --enable-cma \
       --enable-devel-headers \
       --with-cuda=/usr/local/cuda \
       --with-verbs \
       --with-dm \
       --with-gdrcopy=/usr/local \
       --enable-mt \
       --with-mlx5-dv && \
    make -j && \
    make install-strip && \
    ldconfig

ENV LD_LIBRARY_PATH=/usr/lib:$LD_LIBRARY_PATH
ENV CPATH=/usr/include:$CPATH
ENV PATH=/usr/bin:$PATH
ENV PKG_CONFIG_PATH=/usr/lib/pkgconfig:$PKG_CONFIG_PATH
SHELL ["/bin/bash","-c"]

WORKDIR /workspace

ENV LD_LIBRARY_PATH=/usr/local/ompi/lib:$LD_LIBRARY_PATH
ENV CPATH=/usr/local/ompi/include:$CPATH
ENV PATH=/usr/local/ompi/bin:$PATH
ENV PKG_CONFIG_PATH=/usr/local/ompi/lib/pkgconfig:$PKG_CONFIG_PATH

#
# Copy NIXL source from the earlier nixl_base stage
#
COPY --from=nixl_base /opt/nixl /opt/nixl
COPY --from=nixl_base /opt/nixl/commit.txt /opt/nixl/commit.txt
RUN cd /opt/nixl && \
    mkdir build && \
    meson setup build/ --prefix=/usr/local/nixl && \
    cd build && \
    ninja && \
    ninja install

# Adjust plugin path for x86 vs. aarch64
RUN if [ "$ARCH" = "amd64" ]; then \
      ln -s /usr/local/nixl/lib/x86_64-linux-gnu /usr/local/nixl/lib_arch; \
    else \
      ln -s /usr/local/nixl/lib/aarch64-linux-gnu /usr/local/nixl/lib_arch; \
    fi

ENV LD_LIBRARY_PATH=/usr/local/nixl/lib_arch/:$LD_LIBRARY_PATH
ENV PYTHONPATH=/usr/local/nixl/lib/python3/dist-packages/:/opt/nixl/test/python/:$PYTHONPATH
ENV UCX_TLS=^cuda_ipc
ENV NIXL_PLUGIN_DIR=/usr/local/nixl/lib_arch/plugins

RUN ls -l /usr/local/nixl/
RUN ls /opt/nixl

#
# Additional utilities (shared)
#
RUN apt update -y && apt install -y git wget curl nvtop tmux vim

# Install nats: different .deb for x86 vs. arm
RUN if [ "$ARCH" = "amd64" ]; then \
      wget --tries=3 --waitretry=5 https://github.com/nats-io/nats-server/releases/download/v2.10.24/nats-server-v2.10.24-amd64.deb && \
      dpkg -i nats-server-v2.10.24-amd64.deb && rm nats-server-v2.10.24-amd64.deb; \
    else \
      wget --tries=3 --waitretry=5 https://github.com/nats-io/nats-server/releases/download/v2.10.24/nats-server-v2.10.24-arm64.deb && \
      dpkg -i nats-server-v2.10.24-arm64.deb && rm nats-server-v2.10.24-arm64.deb; \
    fi

# etcd: different tarball for x86 vs. arm
ENV ETCD_VERSION="v3.5.18"
RUN if [ "$ARCH" = "amd64" ]; then \
      wget --tries=3 --waitretry=5 https://github.com/etcd-io/etcd/releases/download/$ETCD_VERSION/etcd-$ETCD_VERSION-linux-amd64.tar.gz -O /tmp/etcd.tar.gz; \
    else \
      wget --tries=3 --waitretry=5 https://github.com/etcd-io/etcd/releases/download/$ETCD_VERSION/etcd-$ETCD_VERSION-linux-arm64.tar.gz -O /tmp/etcd.tar.gz; \
    fi && \
    mkdir -p /usr/local/bin/etcd && \
    tar -xvf /tmp/etcd.tar.gz -C /usr/local/bin/etcd --strip-components=1 && \
    rm /tmp/etcd.tar.gz
ENV PATH=/usr/local/bin/etcd/:$PATH


### VIRTUAL ENVIRONMENT SETUP ###

# Install uv & create virtualenv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/
RUN mkdir /opt/dynamo && \
    uv venv /opt/dynamo/venv --python 3.12

ENV VIRTUAL_ENV=/opt/dynamo/venv
ENV PATH="${VIRTUAL_ENV}/bin:${PATH}"

#
# vLLM patch & install: skip on ARM for now
#
ARG VLLM_REF="0.8.4"
ARG VLLM_PATCH="vllm_v${VLLM_REF}-dynamo-kv-disagg-patch.patch"
ARG VLLM_PATCHED_PACKAGE_NAME="ai_dynamo_vllm"
ARG VLLM_PATCHED_PACKAGE_VERSION="0.8.4"

RUN if [ "$ARCH" = "amd64" ]; then \
    echo "Building and patching vLLM for x86_64..." && \
    mkdir /tmp/vllm && \
    uv pip install pip wheel && \
    python -m pip download --only-binary=:all: --no-deps --dest /tmp/vllm vllm==v${VLLM_REF} && \
    cd /tmp/vllm && \
    wheel unpack *.whl && \
    cd vllm-${VLLM_REF}/ && \
    patch -p1 < /tmp/deps/vllm/${VLLM_PATCH} && \
    mv vllm-${VLLM_REF}.dist-info ${VLLM_PATCHED_PACKAGE_NAME}-${VLLM_PATCHED_PACKAGE_VERSION}.dist-info && \
    sed -i "s/^Name: vllm/Name: ${VLLM_PATCHED_PACKAGE_NAME}/g" ${VLLM_PATCHED_PACKAGE_NAME}-${VLLM_PATCHED_PACKAGE_VERSION}.dist-info/METADATA && \
    sed -i "s/^Version: ${VLLM_REF}/Version: ${VLLM_PATCHED_PACKAGE_VERSION}/g" ${VLLM_PATCHED_PACKAGE_NAME}-${VLLM_PATCHED_PACKAGE_VERSION}.dist-info/METADATA && \
    sed -i 's/Tag: cp38-abi3-linux_x86_64/Tag: cp38-abi3-manylinux1_x86_64/g' ${VLLM_PATCHED_PACKAGE_NAME}-${VLLM_PATCHED_PACKAGE_VERSION}.dist-info/WHEEL && \
    sed -i "s/-cp38-abi3-linux_x86_64.whl/-cp38-abi3-manylinux1_x86_64.whl/g" ${VLLM_PATCHED_PACKAGE_NAME}-${VLLM_PATCHED_PACKAGE_VERSION}.dist-info/RECORD && \
    mkdir -p /workspace/dist && \
    wheel pack . --dest-dir /workspace/dist && \
    uv pip install /workspace/dist/${VLLM_PATCHED_PACKAGE_NAME}-*.whl; \
  else \
    echo "Skipping vLLM installation on ARM64 (no official binary wheels)"; \
  fi


# Common dependencies
RUN --mount=type=bind,source=./container/deps/requirements.txt,target=/tmp/requirements.txt \
    uv pip install --requirement /tmp/requirements.txt

# Test dependencies
RUN --mount=type=bind,source=./container/deps/requirements.test.txt,target=/tmp/requirements.txt \
    uv pip install --requirement /tmp/requirements.txt

# Finish pyright
RUN pyright --help > /dev/null 2>&1

# Git config
RUN printf "[safe]\n      directory=/workspace\n" > /root/.gitconfig

# unify /bin/sh => /bin/bash
RUN ln -sf /bin/bash /bin/sh

### RUST Setup
RUN apt update -y && \
    apt install --no-install-recommends -y \
      build-essential \
      protobuf-compiler \
      cmake \
      libssl-dev \
      pkg-config

ARG RUST_VERSION=1.86.0
RUN if [ "$ARCH" = "amd64" ]; then \
      export RUSTARCH="x86_64-unknown-linux-gnu" && \
      wget --tries=3 --waitretry=5 "https://static.rust-lang.org/rustup/archive/1.28.1/${RUSTARCH}/rustup-init" && \
      echo "a3339fb004c3d0bb9862ba0bce001861fe5cbde9c10d16591eb3f39ee6cd3e7f *rustup-init" | sha256sum -c - && \
      chmod +x rustup-init && \
      ./rustup-init -y --no-modify-path --profile minimal --default-toolchain $RUST_VERSION --default-host ${RUSTARCH} && \
      rm rustup-init && \
      chmod -R a+w /usr/local/rustup /usr/local/cargo; \
    else \
      export RUSTARCH="aarch64-unknown-linux-gnu" && \
      wget --tries=3 --waitretry=5 "https://static.rust-lang.org/rustup/archive/1.28.1/${RUSTARCH}/rustup-init" && \
      echo "c64b33db2c6b9385817ec0e49a84bcfe018ed6e328fe755c3c809580cc70ce7a *rustup-init" | sha256sum -c - && \
      chmod +x rustup-init && \
      ./rustup-init -y --no-modify-path --profile minimal --default-toolchain $RUST_VERSION --default-host ${RUSTARCH} && \
      rm rustup-init && \
      chmod -R a+w /usr/local/rustup /usr/local/cargo; \
    fi


#######################################
########## Local Dev Image ###########
#######################################
FROM base AS local-dev

ENV USERNAME=ubuntu
ARG USER_UID=1000
ARG USER_GID=1000

RUN apt-get update && apt-get install -y sudo gnupg2 gnupg1 \
    && echo "$USERNAME ALL=(root) NOPASSWD:ALL" > /etc/sudoers.d/$USERNAME \
    && chmod 0440 /etc/sudoers.d/$USERNAME \
    && mkdir -p /home/$USERNAME \
    && chown -R $USERNAME:$USERNAME /home/$USERNAME \
    && rm -rf /var/lib/apt/lists/* \
    && chsh -s /bin/bash $USERNAME

# Copy the venv from base to local-dev, preserving user permissions:
COPY --from=base --chown=$USER_UID:$USER_GID /opt/dynamo/venv/ /opt/dynamo/venv/
RUN chown $USER_UID:$USER_GID /opt/dynamo/venv
COPY --from=base --chown=$USER_UID:$USER_GID /usr/local/bin /usr/local/bin

USER $USERNAME
ENV HOME=/home/$USERNAME
WORKDIR $HOME

RUN SNIPPET="export PROMPT_COMMAND='history -a' && export HISTFILE=$HOME/.commandhistory/.bash_history" \
    && mkdir -p $HOME/.commandhistory \
    && touch $HOME/.commandhistory/.bash_history \
    && echo "$SNIPPET" >> "$HOME/.bashrc"

RUN mkdir -p /home/$USERNAME/.cache/

# vLLM KV path
ENV VLLM_KV_CAPI_PATH=$HOME/dynamo/.build/target/debug/libdynamo_llm_capi.so

ENTRYPOINT ["/opt/nvidia/nvidia_entrypoint.sh"]


##################################
########## Build Image ###########
##################################
FROM base AS build

WORKDIR /workspace

# Copy Python/Cargo config
COPY pyproject.toml /workspace/
COPY README.md /workspace/
COPY LICENSE /workspace/
COPY Cargo.toml /workspace/
COPY Cargo.lock /workspace/
COPY rust-toolchain.toml /workspace/
COPY hatch_build.py /workspace/

COPY lib/ /workspace/lib/
COPY components /workspace/components
COPY launch /workspace/launch

ARG CARGO_BUILD_JOBS
ENV CARGO_BUILD_JOBS=${CARGO_BUILD_JOBS:-16}
ENV CARGO_TARGET_DIR=/workspace/target

RUN cargo build --release --locked --features mistralrs,sglang,vllm,python && \
    cargo doc --no-deps && \
    cp target/release/dynamo-run /usr/local/bin && \
    cp target/release/http /usr/local/bin && \
    cp target/release/llmctl /usr/local/bin && \
    cp target/release/metrics /usr/local/bin && \
    cp target/release/mock_worker /usr/local/bin

COPY deploy/dynamo/sdk /workspace/deploy/dynamo/sdk
COPY deploy/dynamo/api-store /workspace/deploy/dynamo/api-store

ENV VLLM_KV_CAPI_PATH="/opt/dynamo/bindings/lib/libdynamo_llm_capi.so"

# Launch banner
RUN --mount=type=bind,source=./container/launch_message.txt,target=/workspace/launch_message.txt \
    sed '/^#\s/d' /workspace/launch_message.txt > ~/.launch_screen && \
    echo "cat ~/.launch_screen" >> ~/.bashrc

CMD []


###################################
####### WHEEL BUILD STAGE #########
###################################
FROM env_reader AS wheel_env
ARG SELECTED_MANYLINUX_IMAGE

FROM ${SELECTED_MANYLINUX_IMAGE} AS wheel_builder

ARG CARGO_BUILD_JOBS
ENV CARGO_BUILD_JOBS=${CARGO_BUILD_JOBS:-16}
ARG RELEASE_BUILD
WORKDIR /workspace

RUN yum update -y \
    && yum install -y protobuf-compiler \
    || yum install -y \
       https://raw.repo.almalinux.org/almalinux/8.10/AppStream/x86_64/os/Packages/protobuf-3.5.0-15.el8.x86_64.rpm \
       https://raw.repo.almalinux.org/almalinux/8.10/AppStream/x86_64/os/Packages/protobuf-compiler-3.5.0-15.el8.x86_64.rpm \
    && yum clean all \
    && rm -rf /var/cache/yum

ENV RUSTUP_HOME=/usr/local/rustup \
    CARGO_HOME=/usr/local/cargo \
    PATH=/usr/local/cargo/bin:$PATH \
    CARGO_TARGET_DIR=/workspace/target

COPY --from=build /workspace /workspace
COPY --from=build $RUSTUP_HOME $RUSTUP_HOME
COPY --from=build $CARGO_HOME $CARGO_HOME

RUN mkdir /opt/dynamo && \
    uv venv /opt/dynamo/venv --python 3.12
ENV VIRTUAL_ENV=/opt/dynamo/venv
ENV PATH="${VIRTUAL_ENV}/bin:${PATH}"
ENV PATH="$PATH:/usr/local/bin"

# Build the python wheels
RUN source /opt/dynamo/venv/bin/activate && \
    cd /workspace/lib/bindings/python && \
    uv build --wheel --out-dir /workspace/dist --python 3.12 && \
    if [ "$RELEASE_BUILD" = "true" ]; then \
        uv build --wheel --out-dir /workspace/dist --python 3.11 && \
        uv build --wheel --out-dir /workspace/dist --python 3.10; \
    fi && \
    cd /workspace && \
    uv build --wheel --out-dir /workspace/dist


#######################################
########## CI Minimum Image ###########
#######################################
FROM build AS ci_minimum
ENV DYNAMO_HOME=/workspace

COPY . /workspace
COPY --from=wheel_builder /workspace/dist/ /workspace/dist/

RUN mkdir -p /opt/dynamo/bindings/wheels && \
    mkdir /opt/dynamo/bindings/lib && \
    cp dist/ai_dynamo*cp312*.whl /opt/dynamo/bindings/wheels/ || true && \
    cp target/release/libdynamo_llm_capi.so /opt/dynamo/bindings/lib/ || true && \
    cp -r lib/bindings/c/include /opt/dynamo/bindings/.

RUN uv pip install /workspace/dist/ai_dynamo_runtime*cp312*.whl || true
RUN uv pip install /workspace/dist/ai_dynamo*any.whl || true


##########################################
########## Perf Analyzer Image ###########
##########################################
FROM env_reader AS perf_env
ARG SELECTED_BASE_IMAGE
ARG SELECTED_BASE_IMAGE_TAG
ARG GENAI_PERF_TAG

FROM ${SELECTED_BASE_IMAGE}:${SELECTED_BASE_IMAGE_TAG} AS perf_analyzer

ARG GENAI_PERF_TAG
WORKDIR /workspace

RUN apt-get update -y && apt-get -y install cmake g++ libssl-dev python3 rapidjson-dev zlib1g-dev
RUN git clone https://github.com/triton-inference-server/perf_analyzer.git
RUN git -C perf_analyzer checkout ${GENAI_PERF_TAG}
RUN mkdir perf_analyzer/build
RUN cmake -B perf_analyzer/build -S perf_analyzer -D TRITON_ENABLE_PERF_ANALYZER_OPENAI=ON
RUN cmake --build perf_analyzer/build -- -j8
RUN mkdir bin && cp -r perf_analyzer/build/perf_analyzer/src/perf-analyzer-build /workspace/bin/


########################################
########## Development Image ###########
########################################
FROM ci_minimum AS dev
ARG GENAI_PERF_TAG

# Copy Perf Analyzer
COPY --from=perf_analyzer /workspace/bin/perf-analyzer-build/ /perf/bin
COPY --from=perf_analyzer /workspace/perf_analyzer /perf_analyzer
ENV PATH="/perf/bin:${PATH}"

# Install genai-perf for benchmarking
RUN uv pip install "git+https://github.com/triton-inference-server/perf_analyzer.git@${GENAI_PERF_TAG}#subdirectory=genai-perf"
RUN uv pip uninstall -y tritonclient || true

COPY . /workspace

ENTRYPOINT ["/opt/nvidia/nvidia_entrypoint.sh"]
CMD []


####################################
########## Runtime Image ###########
####################################
FROM env_reader AS runtime_env
ARG SELECTED_RUNTIME_IMAGE
ARG SELECTED_RUNTIME_IMAGE_TAG

FROM ${SELECTED_RUNTIME_IMAGE}:${SELECTED_RUNTIME_IMAGE_TAG} AS runtime

WORKDIR /workspace
ENV DYNAMO_HOME=/workspace
ENV VIRTUAL_ENV=/opt/dynamo/venv

# Copy NIXL from build
COPY --from=build /usr/local/nixl /usr/local/nixl
# x86 vs arm path
ARG ARCH
RUN if [ "$ARCH" = "amd64" ]; then \
      ln -s /usr/local/nixl/lib/x86_64-linux-gnu /usr/local/nixl/lib_arch; \
    else \
      ln -s /usr/local/nixl/lib/aarch64-linux-gnu /usr/local/nixl/lib_arch; \
    fi

ENV LD_LIBRARY_PATH=/usr/local/nixl/lib_arch/:$LD_LIBRARY_PATH
ENV PYTHONPATH=/usr/local/nixl/lib/python3/dist-packages/:/opt/nixl/test/python/:$PYTHONPATH

COPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends python3-dev && \
    rm -rf /var/lib/apt/lists/* && \
    uv venv $VIRTUAL_ENV --python 3.12 && \
    echo "source $VIRTUAL_ENV/bin/activate" >> ~/.bashrc

# Copy wheels from wheel_builder
COPY --from=wheel_builder /workspace/dist/*.whl wheelhouse/ || true
# Attempt to install ai-dynamo[vllm] if present
RUN uv pip install ai-dynamo[vllm] --find-links wheelhouse || true && \
    rm -rf wheelhouse

ENV VLLM_KV_CAPI_PATH="/opt/dynamo/bindings/lib/libdynamo_llm_capi.so"

# Launch banner
RUN --mount=type=bind,source=./container/launch_message.txt,target=/workspace/launch_message.txt \
    sed '/^#\s/d' /workspace/launch_message.txt > ~/.launch_screen && \
    echo "cat ~/.launch_screen" >> ~/.bashrc

# Copy examples
COPY ./examples examples/

ENTRYPOINT [ "/usr/bin/bash" ]
CMD []
